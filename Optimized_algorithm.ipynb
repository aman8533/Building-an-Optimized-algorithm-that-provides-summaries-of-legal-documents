{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj4QwcJqnPMgGiHSlHZPtj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aman8533/Building-an-Optimized-algorithm-that-provides-summaries-of-legal-documents/blob/main/Optimized_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k93RH5qXgDU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c4ff550a-e284-455b-8081-14571887859d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 9.5 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 18.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.29)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 48.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 48.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 45.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 41.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 47.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 47.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 46.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 49.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 46.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 45.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 51.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 51.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f75c0c3821690671a2c3a55630cc1a308daf045e330f6b5a5b9f4ab9d84fa103\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: shortuuid, setproctitle, sentry-sdk, pathtools, docker-pycreds, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 wandb-0.13.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.7.3)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading PyPDF2-2.11.1-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from PyPDF2) (4.1.1)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-2.11.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 8.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: Sentencepiece\n",
            "Successfully installed Sentencepiece-0.1.97\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.7.5-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
            "Collecting pdfminer.six==20220524\n",
            "  Downloading pdfminer.six-20220524-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 10.7 MB/s \n",
            "\u001b[?25hCollecting Pillow>=9.1\n",
            "  Downloading Pillow-9.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 42.4 MB/s \n",
            "\u001b[?25hCollecting Wand>=0.6.10\n",
            "  Downloading Wand-0.6.10-py2.py3-none-any.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six==20220524->pdfplumber) (2.1.1)\n",
            "Collecting cryptography>=36.0.0\n",
            "  Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 42.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=36.0.0->pdfminer.six==20220524->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20220524->pdfplumber) (2.21)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Installing collected packages: cryptography, Wand, Pillow, pdfminer.six, pdfplumber\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "Successfully installed Pillow-9.3.0 Wand-0.6.10 cryptography-38.0.1 pdfminer.six-20220524 pdfplumber-0.7.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (9.3.0)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Installing collected packages: fonttools, matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed fonttools-4.38.0 matplotlib-3.5.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytextrank\n",
            "  Downloading pytextrank-3.2.4-py3-none-any.whl (30 kB)\n",
            "Collecting pygments>=2.7.4\n",
            "  Downloading Pygments-2.13.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pytextrank) (1.7.3)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank) (3.4.2)\n",
            "Requirement already satisfied: networkx[default]>=2.6 in /usr/local/lib/python3.7/dist-packages (from pytextrank) (2.6.3)\n",
            "Collecting graphviz>=0.13\n",
            "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting icecream>=2.1\n",
            "  Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Collecting colorama>=0.3.9\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting executing>=0.3.1\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.0.1\n",
            "  Downloading asttokens-2.1.0-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from networkx[default]>=2.6->pytextrank) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.7/dist-packages (from networkx[default]>=2.6->pytextrank) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.7/dist-packages (from networkx[default]>=2.6->pytextrank) (3.5.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (9.3.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (4.38.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1->networkx[default]>=2.6->pytextrank) (2022.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (3.3.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.11.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (8.1.5)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (0.4.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (1.10.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (3.0.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (1.0.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (4.64.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0->pytextrank) (3.9.0)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0->pytextrank) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.0.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->pytextrank) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->pytextrank) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->pytextrank) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0->pytextrank) (2.0.1)\n",
            "Installing collected packages: pygments, executing, colorama, asttokens, icecream, graphviz, pytextrank\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\n",
            "Successfully installed asttokens-2.1.0 colorama-0.4.6 executing-1.2.0 graphviz-0.20.1 icecream-2.1.3 pygments-2.13.0 pytextrank-3.2.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pygments"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "\n",
            "Usage:   \n",
            "  pip3 install [options] <requirement specifier> [package-index-options] ...\n",
            "  pip3 install [options] -r <requirements file> [package-index-options] ...\n",
            "  pip3 install [options] [-e] <vcs project url> ...\n",
            "  pip3 install [options] [-e] <local project path> ...\n",
            "  pip3 install [options] <archive url/path> ...\n",
            "\n",
            "no such option: -m\n",
            "2022-10-29 16:23:02.194064: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.1) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2022-10-29 16:23:14.863078: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.4.1/en_core_web_lg-3.4.1-py3-none-any.whl (587.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 587.7 MB 15 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-lg==3.4.1) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.6.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-lg==3.4.1) (2.0.1)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.4.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "! pip install numpy\n",
        "! pip install wandb\n",
        "! pip install transformers\n",
        "! pip install seaborn\n",
        "! pip install PyPDF2\n",
        "! pip install Sentencepiece\n",
        "! pip install pdfplumber transformers\n",
        "! pip install spacy\n",
        "! pip install -U matplotlib\n",
        "! pip install pytextrank\n",
        "! pip install pandas\n",
        "! pip install spacy python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download en_core_web_lg\n",
        "import PIL.Image\n",
        "if not hasattr(PIL.Image, 'Resampling'):  # Pillow<9.0\n",
        "  PIL.Image.Resampling = PIL.Image\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pytextrank\n",
        "! pip install pandas\n",
        "! pip install matplotlib\n",
        "! pip install datasets"
      ],
      "metadata": {
        "id": "NKYBVu3GXkhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6b5fb3-e9a6-4f77-f0a3-cfe8f38a6947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytextrank in /usr/local/lib/python3.7/dist-packages (3.2.4)\n",
            "Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.7/dist-packages (from pytextrank) (3.4.2)\n",
            "Requirement already satisfied: graphviz>=0.13 in /usr/local/lib/python3.7/dist-packages (from pytextrank) (0.20.1)\n",
            "Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.7/dist-packages (from pytextrank) (2.13.0)\n",
            "Requirement already satisfied: networkx[default]>=2.6 in /usr/local/lib/python3.7/dist-packages (from pytextrank) (2.6.3)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.7/dist-packages (from pytextrank) (1.7.3)\n",
            "Requirement already satisfied: icecream>=2.1 in /usr/local/lib/python3.7/dist-packages (from pytextrank) (2.1.3)\n",
            "Requirement already satisfied: colorama>=0.3.9 in /usr/local/lib/python3.7/dist-packages (from icecream>=2.1->pytextrank) (0.4.6)\n",
            "Requirement already satisfied: asttokens>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from icecream>=2.1->pytextrank) (2.0.8)\n",
            "Requirement already satisfied: executing>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from icecream>=2.1->pytextrank) (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.1->icecream>=2.1->pytextrank) (1.15.0)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.7/dist-packages (from networkx[default]>=2.6->pytextrank) (3.5.3)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.7/dist-packages (from networkx[default]>=2.6->pytextrank) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from networkx[default]>=2.6->pytextrank) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (21.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (4.38.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (9.2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.3->networkx[default]>=2.6->pytextrank) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1->networkx[default]>=2.6->pytextrank) (2022.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (4.64.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (1.10.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (8.1.5)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (0.4.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.0->pytextrank) (2.4.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.0->pytextrank) (3.9.0)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.0->pytextrank) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0->pytextrank) (2022.9.24)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->pytextrank) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0->pytextrank) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0->pytextrank) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.0->pytextrank) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.5.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (9.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (4.38.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.6.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch==1.5.0"
      ],
      "metadata": {
        "id": "zG7ijeJ2XsX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3695c4-ae1c-4cd0-f064-c30b19aa3028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0) (1.21.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-transformers"
      ],
      "metadata": {
        "id": "KxwK9q52Xt86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d987c21f-4840-4815-b448-8a325e0777bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2022.6.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.1.97)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.0.53)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.64.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.25.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: botocore<1.29.0,>=1.28.4 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (1.28.4)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (1.0.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.4->boto3->pytorch-transformers) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.4->boto3->pytorch-transformers) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.0,>=1.28.4->boto3->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Bert, T5, Pegasos, TextRank models \n",
        "!pip install rouge\n",
        "from transformers import pipeline\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import spacy\n",
        "import pytextrank\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import PegasusForConditionalGeneration\n",
        "from transformers import PegasusTokenizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import torch\n",
        "from rouge import Rouge\n",
        "\n",
        "#Summarization of text. Potential models that I will use are: TextRank, LexRank, Latent Semantic \n",
        "#Analysis, T5 transformers, BART transformers, GPT-2 Transformers, XLM Transformers. PEGASUS,\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Text Rank Module Start - Extractive Summary\n",
        "text = \"\"\"One month after the United States began what has become a troubled rollout of a national COVID vaccination campaign, the effort is finally gathering real steam.\n",
        "Close to a million doses -- over 951,000, to be more exact -- made their way into the arms of Americans in the past 24 hours, the U.S. Centers for Disease Control and Prevention reported Wednesday. That's the largest number of shots given in one day since the rollout began and a big jump from the previous day, when just under 340,000 doses were given, CBS News reported.\n",
        "That number is likely to jump quickly after the federal government on Tuesday gave states the OK to vaccinate anyone over 65 and said it would release all the doses of vaccine it has available for distribution. Meanwhile, a number of states have now opened mass vaccination sites in an effort to get larger numbers of people inoculated, CBS News reported.\"\"\"\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "nlp.add_pipe(\"textrank\")\n",
        "doc = nlp(text)\n",
        "for sent in doc._.textrank.summary(limit_phrases=2, limit_sentences=2):\n",
        "      print(sent)\n",
        "Phrases_and_ranks = [(phrase.chunks[0], phrase.rank) for phrase in doc._.phrases]\n",
        "Phrases_and_ranks[:10]\n",
        "print (\"Text Rank: \", Phrases_and_ranks)\n",
        "# Text Rank Module End\n",
        "\n",
        "# T5 Model Start - Abstraction Encoding/Decoding\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
        "text = \"\"\"One month after the United States began what has become a troubled rollout of a national COVID vaccination campaign, the effort is finally gathering real steam.\n",
        "Close to a million doses -- over 951,000, to be more exact -- made their way into the arms of Americans in the past 24 hours, the U.S. Centers for Disease Control and Prevention reported Wednesday. That's the largest number of shots given in one day since the rollout began and a big jump from the previous day, when just under 340,000 doses were given, CBS News reported.\n",
        "That number is likely to jump quickly after the federal government on Tuesday gave states the OK to vaccinate anyone over 65 and said it would release all the doses of vaccine it has available for distribution. Meanwhile, a number of states have now opened mass vaccination sites in an effort to get larger numbers of people inoculated, CBS News reported.\"\"\"\n",
        "summary_text_T5 = summarizer(text, max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
        "print('T5 Model Text Summary: ',summary_text_T5)\n",
        "\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "#human_summary_text = \"One month after troubled covid vaccination campaign began, over 951,000 people have been vaccinated so far\" # Test Text to check the summarization loss\n",
        "human_summary_text = \"Close to a million doses have been given in the past 24 hours, the cdc says . That's the largest number of shots given in one day since the rollout began.\"\n",
        "input_ids = tokenizer(summary_text_T5, return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(human_summary_text, return_tensors=\"pt\").input_ids\n",
        "# the forward function automatically creates the correct decoder_input_ids\n",
        "loss = model(input_ids=input_ids, labels=labels).loss\n",
        "loss.item() #Loss calculation\n",
        "# T5 Model End \n",
        "\n",
        "\n",
        "# Define PEGASUS model\n",
        "model_name = \"google/pegasus-xsum\"\n",
        "pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "# Create tokens\n",
        "tokens = pegasus_tokenizer(text, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "# Summarize text\n",
        "encoded_summary = pegasus_model.generate(**tokens)\n",
        "# Decode summarized text\n",
        "decoded_summary = pegasus_tokenizer.decode ( encoded_summary[0],skip_special_tokens=True)\n",
        "# Define summarization pipeline \n",
        "summarizer = pipeline(\"summarization\", model=model_name, tokenizer=pegasus_tokenizer, framework=\"pt\")\n",
        "summary_text_Pegasus = summarizer(text, max_length=100, min_length=5, do_sample=False)[0]['summary_text']\n",
        "print('PEGASUS Model Text Summary: ',summary_text_Pegasus)\n",
        "\n",
        "input_ids = pegasus_tokenizer(summary_text_Pegasus, return_tensors=\"pt\").input_ids\n",
        "labels = tokenizer(human_summary_text, return_tensors=\"pt\").input_ids\n",
        "# the forward function automatically creates the correct decoder_input_ids\n",
        "loss = model(input_ids=input_ids, labels=labels).loss\n",
        "loss.item() #Loss calculation\n",
        "\n",
        "# T5 Model End \n",
        "\n",
        "#Bart Model\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer_bart = summarizer(text, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "print(\"Bart summarization:\",summarizer_bart)\n",
        "\n",
        "#Bert Sentence Embedding Model\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\",output_hidden_states = True)\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "hidden_states = outputs.hidden_states\n",
        "token_vecs = hidden_states[-2][0]\n",
        "# Calculate the average of all 22 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "print('Bert Summary: ',sentence_embedding)\n",
        "\n",
        "ROUGE = Rouge()\n",
        "print (\"Bart Evaluation\")\n",
        "ROUGE.get_scores(summarizer_bart, human_summary_text)\n"
      ],
      "metadata": {
        "id": "Z37Re93qXvlx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950
        },
        "outputId": "702c355c-5025-4ac1-b2ec-d72a912973fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Close to a million doses -- over 951,000, to be more exact -- made their way into the arms of Americans in the past 24 hours, the U.S. Centers for Disease Control and Prevention reported Wednesday.\n",
            "One month after the United States began what has become a troubled rollout of a national COVID vaccination campaign, the effort is finally gathering real steam.\n",
            "\n",
            "Text Rank:  [(Wednesday, 0.13217188489775455), (real steam, 0.10581605533073346), (CBS News, 0.10101091209671928), (Disease Control, 0.0926596375784235), (mass vaccination sites, 0.09203339500703157), (larger numbers, 0.08517674883908094), (distribution, 0.08479563278004669), (Prevention, 0.07154126745257039), (a national COVID vaccination campaign, 0.06615564564021807), (vaccine, 0.06186683568335749), (states, 0.06079909879910578), (Americans, 0.058530200896772536), (Tuesday, 0.05495515369656152), (people, 0.05490951036967184), (the U.S. Centers for Disease Control and Prevention, 0.049369384658843135), (the previous day, 0.0481261581275662), (the U.S. Centers, 0.04686923887465883), (a troubled rollout, 0.04587941525071014), (the largest number, 0.045171792527653146), (shots, 0.04429548429929077), (a big jump, 0.041116577490863775), (an effort, 0.03715537254242374), (the effort, 0.03715537254242374), (the federal government, 0.03524437082485955), (one day, 0.03455578091306768), (the rollout, 0.033935035848269644), (the past 24 hours, 0.031974028014270815), (That number, 0.029920248517446797), (a number, 0.029920248517446797), (the United States, 0.028426301089216347), (all the doses, 0.02785373606980273), (the OK, 0.02599922387401616), (the arms, 0.025536290802279045), (their way, 0.024827238523700276), (just under 340,000 doses, 0.022827348896464414), (Close to a million doses, 0.019772332469656106), (One month, 0.015339877755077093), (65, 0.0), (951,000, 0.0), (Close to a million, 0.0), (That, 0.0), (anyone, 0.0), (it, 0.0), (just under 340,000, 0.0), (what, 0.0)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T5 Model Text Summary:  close to a million doses have been given in the past 24 hours, the cdc says . that's the largest number of shots given in one day since the rollout began . a number of states have opened mass vaccination sites to get more people inoculated .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5.py:174: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1301: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 64 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEGASUS Model Text Summary:  More than a million doses of the H1N1 vaccine have been given to Americans in the past 24 hours.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bart summarization:  Over 951,000 doses of vaccine were given in the past 24 hours . That's the largest number of shots given in one day since the rollout began . The federal government on Tuesday gave states the OK to vaccinate anyone over 65 .\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4c774048b3d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2228\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"position_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         self.register_buffer(\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: register_buffer() got an unexpected keyword argument 'persistent'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 1 Training Algorithm\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "\n",
        "base_url = \"https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/\"\n",
        "dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})\n",
        "\n",
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "dataset[\"train\"][100]\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        " \n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(5))\n",
        "\n",
        "print(small_train_dataset)\n",
        "print(small_eval_dataset)\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
        " \n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\")\n",
        " \n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        " \n",
        "metric = load_metric(\"accuracy\")\n",
        " \n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        " \n",
        "from transformers import TrainingArguments, Trainer\n",
        " \n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")\n",
        " \n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train() # Fine Tuned The Model"
      ],
      "metadata": {
        "id": "wyRUMJaLXybU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 2 Train using Keras API: Convert dataset to tensor flow\n",
        "from transformers import DefaultDataCollator\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Cases/\n",
        "\n",
        "base_url = \"https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/\"\n",
        "path_to_file = \"/content/drive/My Drive/Cases/\"\n",
        "\n",
        "dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})\n",
        "\n",
        "dataset = load_dataset(\"yelp_review_full\")\n",
        "\n",
        "print(\"Dataset 1\",dataset)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\",num_labels=5)\n",
        "\n",
        "def count_matches(labels,preds):\n",
        "  print(labels)\n",
        "  print(preds)\n",
        "  return sum ([1 if label ==pred else 0\n",
        "               for label,pred in zip(labels,preds)\n",
        "               ]\n",
        "              )\n",
        "def tokenize_functionNew(examples):\n",
        "  return tokenizer(examples[\"Input_Text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "def tokenize_functionOld(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "def tokenize_functionCNN(examples):\n",
        "  return tokenizer(examples[\"article\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "def tokenize_functionSummary(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "print(\"dataset is:\",dataset)\n",
        "print(\"dataset is:\",dataset.data)\n",
        "\n",
        "#dataset = load_dataset('csv', data_files={'train': path_to_file + 'CaseSummarization.csv','test': path_to_file + 'CaseSummarizationTest.csv'}, encoding=\"utf-8\")\n",
        "tokenized_datasets = dataset.map(tokenize_functionOld, batched=True)\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(2))\n",
        "#print (\"Training Set 2\", small_train_dataset)\n",
        "#print (\"Eval Set 2\", small_eval_dataset)\n",
        "\n",
        "print (\"Summary\", dataset.data)\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",num_labels=5)\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "tf_train_dataset = small_train_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,)\n",
        "\n",
        "tf_validation_dataset =  small_eval_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "print (\"Tensor Set\", tf_train_dataset)\n",
        "print (\"Tensor Set\", tf_validation_dataset)\n",
        "\n",
        "train_data = [['Percy Jackson is a protagonist','Percy Jackson is a protagonist']]\n",
        "\n",
        "train_df = pd.DataFrame(train_data,columns=[\"input_text\",\"target_text\"])\n",
        "\n",
        "eval_data = [['MB is a protagonist','Percy Jackson is a protagonist']]\n",
        "\n",
        "eval_df = pd.DataFrame(eval_data,columns=[\"input_text\",\"target_text\"])\n",
        "\n",
        "tf.convert_to_tensor(train_df[\"input_text\"])\n",
        "tf.convert_to_tensor(train_df[\"target_text\"])\n",
        "tf.convert_to_tensor(eval_df[\"input_text\"])\n",
        "tf.convert_to_tensor(eval_df[\"target_text\"])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "print(\"TF Train\",tf_train_dataset)\n",
        "print(\"TF Validation\", tf_validation_dataset)\n",
        "\n",
        "print(\"Labels = \",tf_train_dataset.element_spec[0])\n",
        "tf_dict = tf_train_dataset.element_spec[0]\n",
        "tf_labels = tf_dict['labels']\n",
        "print(\"Labels 1 = \",tf_labels)\n",
        "#tf_labels = tf.squeeze(tf_labels, axis = -1)\n",
        "print(\"Labels 2 = \",tf_labels)\n",
        "\n",
        "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)"
      ],
      "metadata": {
        "id": "hHeO8I8-X1gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method 3 Training using PyTorch\n",
        "\n",
        "del model\n",
        "del pytorch_model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        " \n",
        "from torch.utils.data import DataLoader\n",
        " \n",
        "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)\n",
        " \n",
        "from transformers import AutoModelForSequenceClassification\n",
        " \n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-basecased\", num_labels=5)\n",
        " \n",
        "from torch.optim import AdamW\n",
        " \n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        " \n",
        "from transformers import get_scheduler\n",
        " \n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        " \n",
        "import torch\n",
        " \n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        " \n",
        "from tqdm.auto import tqdm\n",
        " \n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        " \n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        " \n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        " \n",
        "metric = load_metric(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        " \n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        " \n",
        "metric.compute()\n"
      ],
      "metadata": {
        "id": "HXZv1ej-X3tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Streamlit UI Implementation\n",
        "#Accepts Legal Cases as a Pdf file parse the data as strings\n",
        "# The UI enables useers to choose different algorithms \n",
        "#Bert T5 GPT TextRank LexRank Pegasos\n",
        "#Compute Rouge Scores and idetify most optimized algorithms"
      ],
      "metadata": {
        "id": "Imi7owC6Mkhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Streamlit installation\n",
        "\n",
        "!pip install streamlit -q\n",
        "!pip install bert-extractive-summarizer\n",
        "!pip install streamlit-option-menu\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Cases/"
      ],
      "metadata": {
        "id": "29oB1fkFX5Vv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92cb3c55-d267-491a-f3bd-76b1c6914847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 9.2 MB 6.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 40.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 164 kB 46.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 46.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 44.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 802 kB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 4.5 MB/s \n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-extractive-summarizer\n",
            "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (3.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (1.0.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.10)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.6.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (8.1.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.10.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy->bert-extractive-summarizer) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy->bert-extractive-summarizer) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->bert-extractive-summarizer) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->bert-extractive-summarizer) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (2.0.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 47.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 73.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2022.6.2)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.10.1 huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting streamlit-option-menu\n",
            "  Downloading streamlit_option_menu-0.3.2-py3-none-any.whl (712 kB)\n",
            "\u001b[K     |████████████████████████████████| 712 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: streamlit>=0.63 in /usr/local/lib/python3.7/dist-packages (from streamlit-option-menu) (1.14.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (7.1.2)\n",
            "Requirement already satisfied: validators>=0.2 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.20.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.8.2)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.1.9)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (12.6.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.5.1)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.3.5)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.8.0b4)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (5.1.1)\n",
            "Requirement already satisfied: blinker>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (7.1.2)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (21.3)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.23.0)\n",
            "Requirement already satisfied: semver in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (2.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.21.6)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (6.0.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (3.17.3)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.2.4)\n",
            "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.13.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (3.1.29)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (4.1.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit>=0.63->streamlit-option-menu) (0.10.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (4.3.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit>=0.63->streamlit-option-menu) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit>=0.63->streamlit-option-menu) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->streamlit>=0.63->streamlit-option-menu) (3.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (5.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (22.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.1->streamlit>=0.63->streamlit-option-menu) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit>=0.63->streamlit-option-menu) (2022.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.12->streamlit>=0.63->streamlit-option-menu) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit>=0.63->streamlit-option-menu) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit>=0.63->streamlit-option-menu) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit>=0.63->streamlit-option-menu) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit>=0.63->streamlit-option-menu) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.4->streamlit>=0.63->streamlit-option-menu) (1.24.3)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.11.0->streamlit>=0.63->streamlit-option-menu) (0.9.1)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich>=10.11.0->streamlit>=0.63->streamlit-option-menu) (2.6.1)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators>=0.2->streamlit>=0.63->streamlit-option-menu) (4.4.2)\n",
            "Installing collected packages: streamlit-option-menu\n",
            "Successfully installed streamlit-option-menu-0.3.2\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Cases\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import pdfplumber\n",
        "#import PyPDF2\n",
        "import pathlib\n",
        "#from transformers import pipeline\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytextrank\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import PegasusForConditionalGeneration\n",
        "from transformers import PegasusTokenizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from rouge import Rouge\n",
        "\n",
        "#Tokenizer\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import AutoTokenizer,BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
        "\n",
        "from streamlit_option_menu import option_menu\n",
        "\n",
        "#from summarizer import Summarizer,TransformerSummarizer\n",
        "#from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "st.write('## Natural Language Processor To Generate Summaries of Legal Cases')\n",
        "st.write('### Input Texts that you want to summarize or choose a file')\n",
        "textCase = st.text_area('Input case as text here:', height=150)  \n",
        "\n",
        "path_to_file = \"/content/drive/My Drive/Cases/\"\n",
        "file_list = os.listdir(path_to_file)\n",
        "chooseSelectBox = st.checkbox('Choose Case File')\n",
        "uploaded_file = st.selectbox(\"Choose a Case file as pdf\", file_list)\n",
        "\n",
        "#Initialize Summary to  \"\" as state variable\n",
        "\n",
        "if 'strSummary' not in st.session_state:\n",
        "    st.session_state.strSummary = \"\"\n",
        "if 'textSummary' not in st.session_state:\n",
        "    st.session_state.textSummary = \"\"\n",
        "if 'caseSummary' not in st.session_state:\n",
        "    st.session_state.caseSummary = \"\"\n",
        "\n",
        "#If SelectBox is chosen\n",
        "if(chooseSelectBox):\n",
        "  if uploaded_file is not None:   #if files are uploaded and chose\n",
        "      uploaded_fileName = uploaded_file\n",
        "      uploaded_fileName = path_to_file+uploaded_fileName\n",
        "      #pdfFileObj = open(uploaded_fileName, 'rb')\n",
        "      #pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "      #documentPage = []\n",
        "      #document = []\n",
        "      #strDataInput = \"\"\n",
        "      \n",
        "      #for page in range(pdfReader.numPages):\n",
        "      #      pageObj = pdfReader.getPage(page)\n",
        "      #      strCleanData = pageObj.extractText() \n",
        "      #      strDataInput = strDataInput+strCleanData\n",
        "      #      documentPage.append(strCleanData)\n",
        "      #document.append(documentPage)\n",
        "      #pdfFileObj.close()\n",
        "\n",
        "      #Read the pdf files either using pdfplumber or PyPDF2 and stores the Data as strDataInput\n",
        "\n",
        "      document=[]\n",
        "      iIndex = 0\n",
        "     \n",
        "      documentPage = []\n",
        "      strDataInput = \"\"\n",
        "      with pdfplumber.open(uploaded_fileName) as pdf:\n",
        "          totalpages = len(pdf.pages)\n",
        "          for i in range(0,totalpages):\n",
        "            extracted_page = pdf.pages[i]\n",
        "            extracted_text = extracted_page.extract_text()\n",
        "            strDataInput = strDataInput+extracted_text\n",
        "            documentPage.append(extracted_text)\n",
        "          document.append(documentPage)\n",
        "          iIndex+=1\n",
        "      pdf.close()\n",
        "else:\n",
        "  strDataInput = textCase\n",
        "\n",
        "\n",
        "#Choose the algorithm and Training Technieque\n",
        "\n",
        "optionAlgo = st.selectbox('Choose the summarization algorithm',('TextRank','Transformer:T5', 'Transformer:Pegasos', 'Transformer:Bert', 'Transformer:Bert2BertCNN','Transformer:Bart','GPT2','XLNET'))\n",
        "optionTraining = st.selectbox('Training Technique',('Autotokenizer','Keras:TensorFlow', 'Pytorch'))\n",
        "buttonClicked = st.button('Generate Summary')\n",
        "\n",
        "\n",
        "#If summary button is chosen\n",
        "\n",
        "if (buttonClicked and uploaded_file is not None):\n",
        "  \n",
        "  #If TextRank is chosen\n",
        "\n",
        "  if (optionAlgo == \"TextRank\"):\n",
        "    nlp = spacy.load(\"en_core_web_lg\") \n",
        "    nlp.add_pipe(\"textrank\")\n",
        "    doc = nlp(strDataInput)\n",
        "    for sent in doc._.textrank.summary(limit_phrases=2, limit_sentences=2):\n",
        "          print(sent)\n",
        "\n",
        "    # Get Phrases_Ranks and choose Top 100\n",
        "    Phrases_and_ranks = [(phrase.chunks[0], phrase.rank) for phrase in doc._.phrases]\n",
        "    st.session_state.strSummary = Phrases_and_ranks[:100]\n",
        "  elif (optionAlgo == \"Transformer:T5\"):      #If optionAlgo is T5\n",
        "    # T5 Model Start - Abstraction Encoding/Decoding\n",
        "    model_name = \"t5-large\"\n",
        "    max_length = 500\n",
        "    min_length = 50\n",
        "\n",
        "    # initialize the model architecture and weights using preTrained T5 model\n",
        "    t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    # initialize the model tokenizer\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    # Use Trained T5 Tokenizer stored in local drive\n",
        "\n",
        "    filename = \"/content/drive/My Drive/Cases/T5Model\" \n",
        "    filenameTok = \"/content/drive/My Drive/Cases/T5Model/Tokenizer\" \n",
        "    from tensorflow import keras\n",
        "    t5_model = keras.models.load_model(filename,compile=False)\n",
        "    t5_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/Cases/T5Model/Tokenizer\")\n",
        "\n",
        "    inputs = t5_tokenizer.encode(\"summarize:\" + strDataInput, return_tensors=\"pt\", max_length=512, \n",
        "    padding='max_length', truncation=True)\n",
        "    summary_ids = t5_model.generate(inputs,num_beams=int(2),no_repeat_ngram_size=3,\n",
        "          length_penalty=2.0,min_length=min_length,max_length=max_length,early_stopping=True)\n",
        "  \n",
        "   # Generate the output\n",
        "\n",
        "    output = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    st.session_state.strSummary = output\n",
        "  elif (optionAlgo == \"Transformer:Pegasos\"):  #If optionAlgo is T5\n",
        "    model_name = \"google/pegasus-xsum\"\n",
        "    pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "    pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "    # Create tokens\n",
        "    tokens = pegasus_tokenizer(strDataInput, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "    # Summarize text\n",
        "    encoded_summary = pegasus_model.generate(**tokens)\n",
        "    # Decode summarized text\n",
        "    decoded_summary = pegasus_tokenizer.decode ( encoded_summary[0],skip_special_tokens=True)\n",
        "    st.session_state.strSummary = decoded_summary\n",
        "  elif(optionAlgo == \"Transformer:Bert\"):  #If optionAlgo is Bert\n",
        "    max_length = 500\n",
        "    min_length = 50\n",
        "    #Bert Sentence Embedding Model\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    bert_model = BertModel.from_pretrained(model_name, output_hidden_states = True ) #output_hidden_states = True\n",
        "    inputs = bert_tokenizer(strDataInput, return_tensors=\"pt\",max_length=512,padding='max_length', truncation=True)\n",
        "    outputs = bert_model(**inputs)\n",
        "    #strSummary = outputs  #hidden_states\n",
        "    #strSummary = bert_tokenizer.decode(outputs[0])\n",
        "    model = Summarizer()\n",
        "    #st.write(model)\n",
        "    result = model(strDataInput, ratio = 0.1)\n",
        "    #st.write(result)\n",
        "    full = ''.join(result)\n",
        "    st.session_state.strSummary = full\n",
        "  elif(optionAlgo == \"Transformer:Bert2BertCNN\"):  #If optionAlgo is Bert2BertCNN\n",
        "    model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
        "    input_ids = tokenizer(strDataInput, return_tensors=\"pt\",max_length=512,padding='max_length', truncation=True).input_ids\n",
        "\n",
        "    # autoregressively generate summary (uses greedy decoding by default)\n",
        "    generated_ids = model.generate(input_ids)\n",
        "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    st.session_state.strSummary = generated_text\n",
        "  elif(optionAlgo == \"GPT2\"):  #If optionAlgo is GPT2\n",
        "    GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\n",
        "    strResult = ''.join(GPT2_model(strDataInput, min_length=60))\n",
        "    st.session_state.strSummary = strResult\n",
        "  elif (optionAlgo == \"XLNET\"): #If optionAlgo is XLNET\n",
        "    model = TransformerSummarizer(transformer_type=\"XLNet\",transformer_model_key=\"xlnet-base-cased\")\n",
        "    strResult = ''.join(model(strDataInput, min_length=60))\n",
        "    st.session_state.strSummary = strResult\n",
        "  else:\n",
        "    st.session_state.strSummary = \"Model Not Implemented Yet\"\n",
        "\n",
        "#Original Text before summarization\n",
        "textOriginal = st.text_area('Original Text',value = strDataInput, height=150)\n",
        "\n",
        "#Post summarization\n",
        "\n",
        "#if (st.session_state.strSummary !=\"\"):\n",
        "#  st.session_state.textSummary = st.text_area('Text Summary',value = st.session_state.strSummary, height=150) \n",
        "#  st.session_state.caseSummary = st.text_area('Actual Case Summary',value = st.session_state.strSummary, height=150)\n",
        "\n",
        "#If Rouge is Clicked\n",
        "rougeClicked = st.button('Generate Rouge Score')\n",
        "\n",
        "#Output Rouge Score\n",
        "\n",
        "if(rougeClicked):\n",
        "  if(st.session_state.textSummary != \"\" and st.session_state.caseSummary != \"\"):\n",
        "    rougeObj = Rouge()\n",
        "    scoreObj = rougeObj.get_scores(st.session_state.textSummary, st.session_state.caseSummary)\n",
        "    score = st.text_area('Rouge Score',value = scoreObj, height=100)"
      ],
      "metadata": {
        "id": "F3bNtX1U2cOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import pdfplumber\n",
        "#import PyPDF2\n",
        "import pathlib\n",
        "#from transformers import pipeline\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytextrank\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import PegasusForConditionalGeneration\n",
        "from transformers import PegasusTokenizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from rouge import Rouge\n",
        "\n",
        "#Tokenizer\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import AutoTokenizer,BertConfig, EncoderDecoderConfig, EncoderDecoderModel\n",
        "\n",
        "from streamlit_option_menu import option_menu\n",
        "\n",
        "#from summarizer import Summarizer,TransformerSummarizer\n",
        "#from transformers import GPT2Tokenizer, GPT2Model\n",
        "\n",
        "st.write('## Natural Language Processor To Generate Summaries of Legal Cases')\n",
        "st.write('### Input Texts that you want to summarize or choose a file')\n",
        "textCase = st.text_area('Input case as text here:', height=150)  \n",
        "\n",
        "path_to_file = \"/content/drive/My Drive/Cases/\"\n",
        "file_list = os.listdir(path_to_file)\n",
        "chooseSelectBox = st.checkbox('Choose Case File')\n",
        "uploaded_file = st.selectbox(\"Choose a Case file as pdf\", file_list)\n",
        "\n",
        "#Initialize Summary to  \"\" as state variable\n",
        "\n",
        "if 'strSummary' not in st.session_state:\n",
        "    st.session_state.strSummary = \"\"\n",
        "if 'textSummary' not in st.session_state:\n",
        "    st.session_state.textSummary = \"\"\n",
        "if 'caseSummary' not in st.session_state:\n",
        "    st.session_state.caseSummary = \"\"\n",
        "\n",
        "#If SelectBox is chosen\n",
        "if(chooseSelectBox):\n",
        "  if uploaded_file is not None:   #if files are uploaded and chose\n",
        "      uploaded_fileName = uploaded_file\n",
        "      uploaded_fileName = path_to_file+uploaded_fileName\n",
        "      #pdfFileObj = open(uploaded_fileName, 'rb')\n",
        "      #pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "      #documentPage = []\n",
        "      #document = []\n",
        "      #strDataInput = \"\"\n",
        "      \n",
        "      #for page in range(pdfReader.numPages):\n",
        "      #      pageObj = pdfReader.getPage(page)\n",
        "      #      strCleanData = pageObj.extractText() \n",
        "      #      strDataInput = strDataInput+strCleanData\n",
        "      #      documentPage.append(strCleanData)\n",
        "      #document.append(documentPage)\n",
        "      #pdfFileObj.close()\n",
        "\n",
        "      #Read the pdf files either using pdfplumber or PyPDF2 and stores the Data as strDataInput\n",
        "\n",
        "      document=[]\n",
        "      iIndex = 0\n",
        "     \n",
        "      documentPage = []\n",
        "      strDataInput = \"\"\n",
        "      with pdfplumber.open(uploaded_fileName) as pdf:\n",
        "          totalpages = len(pdf.pages)\n",
        "          for i in range(0,totalpages):\n",
        "            extracted_page = pdf.pages[i]\n",
        "            extracted_text = extracted_page.extract_text()\n",
        "            strDataInput = strDataInput+extracted_text\n",
        "            documentPage.append(extracted_text)\n",
        "          document.append(documentPage)\n",
        "          iIndex+=1\n",
        "      pdf.close()\n",
        "else:\n",
        "  strDataInput = textCase\n",
        "\n",
        "\n",
        "#Choose the algorithm and Training Technieque\n",
        "\n",
        "optionAlgo = st.selectbox('Choose the summarization algorithm',('TextRank','Transformer:T5', 'Transformer:Pegasos', 'Transformer:Bert', 'Transformer:Bert2BertCNN','Transformer:Bart','GPT2','XLNET'))\n",
        "optionTraining = st.selectbox('Training Technique',('Autotokenizer','Keras:TensorFlow', 'Pytorch'))\n",
        "buttonClicked = st.button('Generate Summary')\n",
        "\n",
        "\n",
        "#If summary button is chosen\n",
        "\n",
        "if (buttonClicked and uploaded_file is not None):\n",
        "  \n",
        "  #If TextRank is chosen\n",
        "\n",
        "  if (optionAlgo == \"TextRank\"):\n",
        "    nlp = spacy.load(\"en_core_web_lg\") \n",
        "    nlp.add_pipe(\"textrank\")\n",
        "    doc = nlp(strDataInput)\n",
        "    for sent in doc._.textrank.summary(limit_phrases=2, limit_sentences=2):\n",
        "          print(sent)\n",
        "\n",
        "    # Get Phrases_Ranks and choose Top 100\n",
        "    Phrases_and_ranks = [(phrase.chunks[0], phrase.rank) for phrase in doc._.phrases]\n",
        "    st.session_state.strSummary = Phrases_and_ranks[:100]\n",
        "  elif (optionAlgo == \"Transformer:T5\"):      #If optionAlgo is T5\n",
        "    # T5 Model Start - Abstraction Encoding/Decoding\n",
        "    model_name = \"t5-large\"\n",
        "    max_length = 500\n",
        "    min_length = 50\n",
        "\n",
        "    # initialize the model architecture and weights using preTrained T5 model\n",
        "    t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "    # initialize the model tokenizer\n",
        "    t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    \n",
        "    # Use Trained T5 Tokenizer stored in local drive\n",
        "\n",
        "    filename = \"/content/drive/My Drive/Cases/T5Model\" \n",
        "    filenameTok = \"/content/drive/My Drive/Cases/T5Model/Tokenizer\" \n",
        "    from tensorflow import keras\n",
        "    t5_model = keras.models.load_model(filename,compile=False)\n",
        "    t5_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/Cases/T5Model/Tokenizer\")\n",
        "\n",
        "    inputs = t5_tokenizer.encode(\"summarize:\" + strDataInput, return_tensors=\"pt\", max_length=512, \n",
        "    padding='max_length', truncation=True)\n",
        "    summary_ids = t5_model.generate(inputs,num_beams=int(2),no_repeat_ngram_size=3,\n",
        "          length_penalty=2.0,min_length=min_length,max_length=max_length,early_stopping=True)\n",
        "  \n",
        "   # Generate the output\n",
        "\n",
        "    output = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    st.session_state.strSummary = output\n",
        "  elif (optionAlgo == \"Transformer:Pegasos\"):  #If optionAlgo is T5\n",
        "    model_name = \"google/pegasus-xsum\"\n",
        "    pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "    pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
        "    # Create tokens\n",
        "    tokens = pegasus_tokenizer(strDataInput, truncation=True, padding=\"longest\", return_tensors=\"pt\")\n",
        "    # Summarize text\n",
        "    encoded_summary = pegasus_model.generate(**tokens)\n",
        "    # Decode summarized text\n",
        "    decoded_summary = pegasus_tokenizer.decode ( encoded_summary[0],skip_special_tokens=True)\n",
        "    st.session_state.strSummary = decoded_summary\n",
        "  elif(optionAlgo == \"Transformer:Bert\"):  #If optionAlgo is Bert\n",
        "    max_length = 500\n",
        "    min_length = 50\n",
        "    #Bert Sentence Embedding Model\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    bert_model = BertModel.from_pretrained(model_name, output_hidden_states = True ) #output_hidden_states = True\n",
        "    inputs = bert_tokenizer(strDataInput, return_tensors=\"pt\",max_length=512,padding='max_length', truncation=True)\n",
        "    outputs = bert_model(**inputs)\n",
        "    #strSummary = outputs  #hidden_states\n",
        "    #strSummary = bert_tokenizer.decode(outputs[0])\n",
        "    model = Summarizer()\n",
        "    #st.write(model)\n",
        "    result = model(strDataInput, ratio = 0.1)\n",
        "    #st.write(result)\n",
        "    full = ''.join(result)\n",
        "    st.session_state.strSummary = full\n",
        "  elif(optionAlgo == \"Transformer:Bert2BertCNN\"):  #If optionAlgo is Bert2BertCNN\n",
        "    model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
        "    input_ids = tokenizer(strDataInput, return_tensors=\"pt\",max_length=512,padding='max_length', truncation=True).input_ids\n",
        "\n",
        "    # autoregressively generate summary (uses greedy decoding by default)\n",
        "    generated_ids = model.generate(input_ids)\n",
        "    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    st.session_state.strSummary = generated_text\n",
        "  elif(optionAlgo == \"GPT2\"):  #If optionAlgo is GPT2\n",
        "    GPT2_model = TransformerSummarizer(transformer_type=\"GPT2\",transformer_model_key=\"gpt2-medium\")\n",
        "    strResult = ''.join(GPT2_model(strDataInput, min_length=60))\n",
        "    st.session_state.strSummary = strResult\n",
        "  elif (optionAlgo == \"XLNET\"): #If optionAlgo is XLNET\n",
        "    model = TransformerSummarizer(transformer_type=\"XLNet\",transformer_model_key=\"xlnet-base-cased\")\n",
        "    strResult = ''.join(model(strDataInput, min_length=60))\n",
        "    st.session_state.strSummary = strResult\n",
        "  else:\n",
        "    st.session_state.strSummary = \"Model Not Implemented Yet\"\n",
        "\n",
        "#Original Text before summarization\n",
        "textOriginal = st.text_area('Original Text',value = strDataInput, height=150)\n",
        "\n",
        "#Post summarization\n",
        "\n",
        "if (st.session_state.strSummary !=\"\"):\n",
        "  st.session_state.textSummary = st.text_area('Text Summary',value = st.session_state.strSummary, height=150) \n",
        "  st.session_state.caseSummary = st.text_area('Actual Case Summary',value = st.session_state.strSummary, height=150)\n",
        "\n",
        "#If Rouge is Clicked\n",
        "rougeClicked = st.button('Generate Rouge Score')\n",
        "\n",
        "#Output Rouge Score\n",
        "\n",
        "if(rougeClicked):\n",
        "  if(st.session_state.textSummary != \"\" and st.session_state.caseSummary != \"\"):\n",
        "    rougeObj = Rouge()\n",
        "    scoreObj = rougeObj.get_scores(st.session_state.textSummary, st.session_state.caseSummary)\n",
        "    score = st.text_area('Rouge Score',value = scoreObj, height=100)"
      ],
      "metadata": {
        "id": "MWsflUVrX_32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fb2341-7252-48d5-db99-5097b5ac9cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyngrok"
      ],
      "metadata": {
        "id": "T1QMSatJYC_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cd27077-9e73-423c-8c12-26e63bfa6280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-5.1.0.tar.gz (745 kB)\n",
            "\u001b[K     |████████████████████████████████| 745 kB 8.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.1.0-py3-none-any.whl size=19007 sha256=18d389bea2b1d3670f315f8e81cfc9d5952dee6f1fedd70d7311d74e4612fdcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/e6/af/ccf6598ecefecd44104069371795cb9b3afbcd16987f6ccfb3\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-5.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "wL252aBVYDGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.set_auth_token(\"2DFRrpyp9EziRs4rpG0wFv79OHg_6NxD8Daqfvu5cZXAmrv9J\") "
      ],
      "metadata": {
        "id": "rpUG8f8dYGfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9663f599-bf7e-4669-95a2-6f849efdb2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup streamlit run app.py --server.port 80 &\n",
        "url = ngrok.connect(port = '80')\n",
        "print(url)"
      ],
      "metadata": {
        "id": "HpX5Syo1YIHF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dbd8491-c91f-465e-903c-b8eefa093c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "NgrokTunnel: \"http://25e5-34-125-172-145.ngrok.io\" -> \"http://localhost:80\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "ExpSl0V1phxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eba7634d-067c-4a5b-9aa9-68703a4bdb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "tunnels=ngrok.get_tunnels()\n",
        "tunnels\n",
        "\n",
        "\n",
        "# try:\n",
        "#     # Block until CTRL-C or some other terminating event\n",
        "#     ngrok_process.proc.wait()\n",
        "# except KeyboardInterrupt:\n",
        "#     print(\" Shutting down server.\")\n",
        "\n",
        "#     ngrok.kill()"
      ],
      "metadata": {
        "id": "f-OE8-OspjKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5c322e-68da-46f1-eca5-4206a37827e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<NgrokTunnel: \"https://25e5-34-125-172-145.ngrok.io\" -> \"http://localhost:80\">,\n",
              " <NgrokTunnel: \"http://25e5-34-125-172-145.ngrok.io\" -> \"http://localhost:80\">]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import pathlib\n",
        "from transformers import pipeline\n",
        "import spacy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytextrank\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import PegasusForConditionalGeneration\n",
        "from transformers import PegasusTokenizer\n",
        "from transformers import BertTokenizer, BertModel,TFBertModel\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from rouge import Rouge\n",
        "from google.colab import drive\n",
        "from summarizer import Summarizer\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Cases/\n",
        "\n",
        "path_to_file = \"SupremeCourtCase1.PDF\"\n",
        "#file_list = os.listdir(path_to_file)\n",
        "#uploaded_file = st.selectbox(\"Or Choose a Case file as pdf\", file_list)\n",
        "uploaded_file=path_to_file\n",
        "if uploaded_file is not None:\n",
        "    #uploaded_fileName = uploaded_file.name\n",
        "    uploaded_fileName = \"SupremeCourtCase1.PDF\"\n",
        "    #uploaded_fileName = path_to_file+uploaded_fileName\n",
        "    print(uploaded_fileName)\n",
        "    pdfFileObj = open(uploaded_fileName, 'rb')\n",
        "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "    documentPage = []\n",
        "    document = []\n",
        "    strDataInput = \"\"\n",
        "    for page in range(pdfReader.numPages):\n",
        "          pageObj = pdfReader.getPage(page)\n",
        "          strCleanData = pageObj.extractText() \n",
        "          strDataInput = strDataInput+strCleanData\n",
        "          documentPage.append(strCleanData)\n",
        "    document.append(documentPage)\n",
        "    pdfFileObj.close()\n",
        "max_length = 500\n",
        "min_length = 50\n",
        "#Bert Sentence Embedding Model\n",
        "model_name = \"bert-base-uncased\"\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = BertModel.from_pretrained(model_name, output_hidden_states = True ) #output_hidden_states = True\n",
        "inputs = bert_tokenizer(strDataInput, return_tensors=\"pt\",max_length=512,padding='max_length', truncation=True)\n",
        "#inputs = bert_tokenizer.encode(\"summarize:\" + strDataInput, return_tensors=\"pt\", max_length=512, \n",
        "#  padding='max_length', truncation=True)\n",
        "\n",
        "outputs = bert_model(**inputs)\n",
        "#summary_ids = bert_model.generate(inputs,num_beams=int(2),no_repeat_ngram_size=3,length_penalty=2.0,\n",
        "#  min_length=min_length,max_length=max_length,early_stopping=True)\n",
        "#output = bert_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "#logits = outputs.logits\n",
        "\n",
        "hidden_states = outputs.hidden_states\n",
        "token_vecs = hidden_states[-2][0]\n",
        "print(token_vecs)\n",
        "#token_vecs = hidden_states[-2][0]\n",
        "# Calculate the average of all 22 token vectors.\n",
        "sentence_embedding = torch.mean(token_vecs, dim=0)\n",
        "print(sentence_embedding)\n",
        "\n",
        "for i,word in enumerate(sentence_embedding):\n",
        "  print(\"The encoding for \",i+1,\"th word is : \\n\\n\",word)\n",
        "\n",
        "model = Summarizer()\n",
        "result = model(strDataInput, min_length=50,max_length = 500)\n",
        "strSummary = result\n",
        "print(\"Summary is:\",strSummary)\n"
      ],
      "metadata": {
        "id": "vyKldTfZplMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training T5 Model using TFAutoModelForSeq2SeqLM\n",
        "\n",
        "from transformers import DefaultDataCollator\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
        "billsum = billsum.train_test_split(test_size=0.2)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_billsum = billsum.map(preprocess_function, batched=True)\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "#data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n",
        "\n",
        "small_train_dataset = tokenized_billsum[\"train\"].shuffle(seed=42).select(range(2))\n",
        "small_eval_dataset = tokenized_billsum[\"test\"].shuffle(seed=42).select(range(2))\n",
        "\n",
        "tf_train_set = tokenized_billsum[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_test_set = small_eval_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "from transformers import create_optimizer, AdamWeightDecay\n",
        "\n",
        "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
        "\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3) "
      ],
      "metadata": {
        "id": "WFM0-ibQpmvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert Training \n",
        "\n",
        "encoded = bert_tokenizer.encode(strDataInput)\n",
        "# encoded: [101, 2070, 3793, 102]\n",
        "decoded = bert_tokenizer.decode(encoded)\n",
        "# decoded: [CLS] some text [SEP]\n",
        "strSummary = decoded  #hidden_states\n",
        "#strSummary = logits\n",
        "\n",
        "encoder = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "## QA Model\n",
        "input_ids = layers.Input(shape=(max_length,), dtype=tf.int32)\n",
        "token_type_ids = layers.Input(shape=(max_length,), dtype=tf.int32)\n",
        "attention_mask = layers.Input(shape=(max_length,), dtype=tf.int32)\n",
        "embedding = encoder(\n",
        "    input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        ")[0]\n",
        "\n",
        "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
        "start_logits = layers.Flatten()(start_logits)\n",
        "\n",
        "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
        "end_logits = layers.Flatten()(end_logits)\n",
        "\n",
        "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
        "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
        "\n",
        "model = keras.Model(\n",
        "    inputs=[input_ids, token_type_ids, attention_mask],\n",
        "    outputs=[start_probs, end_probs],\n",
        ")\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "optimizer = keras.optimizers.Adam(lr=5e-5)\n",
        "model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "strSummary = model.summary()\n",
        "print(\"Summary:\",strSummary)\n",
        "\n",
        "#Roberta Training\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
        "tokenizer.bos_token = tokenizer.cls_token\n",
        "tokenizer.eos_token = tokenizer.sep_token\n",
        "#parameter setting\n",
        "batch_size=256  #\n",
        "encoder_max_length=100\n",
        "decoder_max_length=8\n",
        "config_encoder = BertConfig()\n",
        "config_decoder = BertConfig()\n",
        "config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
        "model = EncoderDecoderModel(config=config)\n",
        "#model = EncoderDecoderModel.from_pretrained(\"roberta-base\", \"roberta-base\")\n",
        "model.to(\"cuda\")\n",
        "batch_size = 1024\n",
        "model.config.decoder_start_token_id = tokenizer.bos_token_id                                             \n",
        "model.config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# sensible parameters for beam search\n",
        "# set decoding params                               \n",
        "model.config.max_length = 100\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4\n",
        "model.config.vocab_size = model.config.encoder.vocab_size\n",
        "# map data correctly\n",
        "def generate_summary(batch):\n",
        "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
        "    inputs = tokenizer(batch[\"Text\"], padding=\"max_length\", truncation=True, max_length=100, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids.to(\"cuda\")\n",
        "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
        "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
        "    # all special tokens including will be removed\n",
        "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    batch[\"pred\"] = output_str\n",
        "    return batch\n",
        "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"Text\"])\n",
        "pred_str = results[\"pred\"]\n",
        "strSummary = results[\"Summary\"]"
      ],
      "metadata": {
        "id": "iQZx4siqpns8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#T5 Training \n",
        "from transformers import DefaultDataCollator\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "from datasets import load_dataset\n",
        "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
        "billsum = billsum.train_test_split(test_size=0.2)\n",
        "billsum[\"train\"][0]\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_billsum = billsum.map(preprocess_function, batched=True)\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n",
        "\n",
        "small_train_dataset = tokenized_billsum[\"train\"].shuffle(seed=42).select(range(5))\n",
        "small_eval_dataset = tokenized_billsum[\"test\"].shuffle(seed=42).select(range(5))\n",
        "\n",
        "\n",
        "tf_train_set = small_train_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_test_set = small_eval_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "from transformers import create_optimizer, AdamWeightDecay\n",
        "\n",
        "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=10)\n"
      ],
      "metadata": {
        "id": "42WgwowcpqvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub\n",
        "from transformers import AutoTokenizer\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "94-EYDorpsg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can of course substitute your own username and model here if you've trained and uploaded it!\n",
        "model_name = \"Amanb2004/t5-small-finetuned-xsum\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "WLp0GZYRptfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "#T5 Training on Billsum database\n",
        "from transformers import DefaultDataCollator\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import PegasusForConditionalGeneration\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
        "billsum = billsum.train_test_split(test_size=0.2)\n",
        "billsum[\"train\"][0]\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_billsum = billsum.map(preprocess_function, batched=True)\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n",
        "\n",
        "small_train_dataset = tokenized_billsum[\"train\"].shuffle(seed=42).select(range(5))\n",
        "small_eval_dataset = tokenized_billsum[\"test\"].shuffle(seed=42).select(range(5))\n",
        "\n",
        "\n",
        "tf_train_set = small_train_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_test_set = small_eval_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "from transformers import create_optimizer, AdamWeightDecay\n",
        "\n",
        "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "#model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=10)\n",
        "\n",
        "filename = \"/content/drive/My Drive/Cases/T5Model\" \n",
        "#filenameTok = \"/content/drive/My Drive/Cases/T5Model/Tokenizer\" \n",
        "#model.save(filename)\n",
        "#tokenizer.save_pretrained(filenameTok)\n",
        "from tensorflow import keras\n",
        "#t5_model = keras.models.load_model(filename,compile=False)\n",
        "strDataInput = \"Hi My name is AB I am Managing Director of PrepSaurus\"\n",
        "#\n",
        "t5_model = model\n",
        "t5_tokenizer = tokenizer\n",
        "max_length = 500\n",
        "min_length = 50\n",
        "\n",
        "t5_model = keras.models.load_model(filename,compile=False)\n",
        "#t5_model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/My Drive/Cases/T5Model\")\n",
        "# initialize the model tokenizer\n",
        "#t5_tokenizer = T5Tokenizer.from_pretrained(\"/content/drive/My Drive/Cases/T5Model/Tokenizer\")\n",
        "\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/Cases/T5Model/Tokenizer\")\n",
        "inputs = t5_tokenizer.encode(\"summarize:\" + strDataInput, return_tensors=\"pt\", max_length=512, \n",
        "padding='max_length', truncation=True)\n",
        "print(\"T5 Model = \",t5_model)\n",
        "summary_ids = t5_model.generate(inputs,num_beams=int(2),no_repeat_ngram_size=3,\n",
        "      length_penalty=2.0,min_length=min_length,max_length=max_length,early_stopping=True)\n",
        "\n",
        "output = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "strSummary=output\n",
        "print(\"Summary:\",strSummary)\n"
      ],
      "metadata": {
        "id": "sJp4RPkRpuvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bert Training on Billsum database\n",
        "from transformers import DefaultDataCollator\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "from datasets import load_dataset\n",
        "billsum = load_dataset(\"billsum\", split=\"ca_test\")\n",
        "billsum = billsum.train_test_split(test_size=0.2)\n",
        "billsum[\"train\"][0]\n",
        "from transformers import AutoTokenizer\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\",num_labels = 5)\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\",num_labels = 5)\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_billsum = billsum.map(preprocess_function, batched=True)\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "small_train_dataset = tokenized_billsum[\"train\"].shuffle(seed=42).select(range(5))\n",
        "small_eval_dataset = tokenized_billsum[\"test\"].shuffle(seed=42).select(range(5))\n",
        "\n",
        "tf_train_set = small_train_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "tf_test_set = small_eval_dataset.to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "#from transformers import create_optimizer, AdamWeightDecay\n",
        "\n",
        "#optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
        "#from transformers import TFAutoModelForSeq2SeqLM\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
        ")\n",
        "\n",
        "#model.compile(optimizer=optimizer)\n",
        "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=10)"
      ],
      "metadata": {
        "id": "_chCjKUTpwBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKFjO47ipxr9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}